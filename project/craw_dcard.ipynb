{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import NavigableString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class crawlerPtt():\n",
    "    def __init__(self,url,pageRange, start_page_id):\n",
    "        # 欲抓取的看板首頁\n",
    "        self.url = url\n",
    "        # 儲存每個標題網址\n",
    "        self.urlList = []\n",
    "        # 儲存每條留言\n",
    "#         self.messageList = []\n",
    "        # 儲存每條留言\n",
    "#         self.titleList = []\n",
    "        self.data = pd.DataFrame()\n",
    "\n",
    "        # 抓取首頁\n",
    "        self.get_all_href(url=self.url)\n",
    "\n",
    "        # 往前幾頁 抓取所有標題網址\n",
    "        for page in range(1, pageRange):\n",
    "            # 使用 GET 方式下載普通網頁\n",
    "            r = requests.get(self.url)\n",
    "            # html.parser 為解析 HTML 文件的模組\n",
    "            soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "            # 抓取按鈕群\n",
    "            btn = soup.select('h2 > a')\n",
    "            # 抓取上一頁按鈕的網址\n",
    "            up_page_href = btn[3]['href']\n",
    "            # 在網址後加上上一頁的網址\n",
    "            next_page_url = 'https://www.ptt.cc' + up_page_href\n",
    "            # 爬取上一頁內容 並更新url成上一頁的網址\n",
    "            self.url = next_page_url\n",
    "            self.get_all_href(url=self.url)\n",
    "\n",
    "        # 透過標題網址self.urlList抓取留言 並儲存到self.messageList最後再存下來 (邊抓邊存會漏掉很多留言)\n",
    "        self.crawlerMessage()\n",
    "\n",
    "    # 抓取標題網址\n",
    "    def get_all_href(self,url):\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        # 抓取文章標題\n",
    "        results = soup.select(\"div.title\")\n",
    "        # 尋訪每個標題 取得網址\n",
    "        for item in results:\n",
    "            a_item = item.select_one(\"a\")\n",
    "            # 標題名稱\n",
    "            title = item.text\n",
    "            if a_item:\n",
    "                \n",
    "                #所要儲存的網站網址\n",
    "                url = 'https://www.ptt.cc' + a_item.get('href')\n",
    "                print(url)\n",
    "                # 儲存網址至佇列\n",
    "                self.urlList.append(url)\n",
    "                \n",
    "    def getResult(self):\n",
    "        return self.data\n",
    "    \n",
    "\n",
    "    # 從標題網址分析留言\n",
    "    def crawlerMessage(self):\n",
    "        # parsing\n",
    "        all_posts = []\n",
    "        for link in self.urlList:\n",
    "            try:\n",
    "                response = requests.get(link).text\n",
    "                soup = BeautifulSoup(response, 'html.parser')\n",
    "            except:\n",
    "                # ignore this link\n",
    "                continue\n",
    "\n",
    "            # meta info\n",
    "            metas = soup.find_all(class_ = 'article-meta-value')\n",
    "            if(len(metas) == 0):\n",
    "                continue\n",
    "            title = metas[-2].text\n",
    "            author = metas[0].text\n",
    "            time = metas[-1].text\n",
    "\n",
    "            # content\n",
    "            content = ''\n",
    "            for text in soup.find(id='main-content'):\n",
    "                if isinstance(text, NavigableString):\n",
    "                    content += text.strip()\n",
    "\n",
    "            # comments(a list of list, each includes push_tag and push_content)\n",
    "            comments = []\n",
    "            for div in soup.find_all(class_ = 'push'):\n",
    "                push_tag, push_content = '', ''\n",
    "                try:\n",
    "                    # push_tag's type is <class 'bs4.element.ResultSet'> (?\n",
    "                    push_tag = div.find('span', 'push-tag').contents[0]\n",
    "                    push_content = div.find('span', 'push-content').text\n",
    "                    push_tag = push_tag.strip()\n",
    "                    push_content = push_content[2:]\n",
    "                except:\n",
    "                    pass\n",
    "                comments.append([push_tag, push_content])\n",
    "\n",
    "            # 將上面的每一篇文章的資訊存成一個 dictionary，並把這些 dicts 存進 all_posts (list)\n",
    "            all_posts.append({'title':title, 'content':content, 'author':author, 'link':link, 'postTime':time, 'comments':comments})\n",
    "        self.data = pd.DataFrame(all_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"tgn9uw-3 iuyCWN\" href=\"/f/relationship/p/234885552\"><span>發現男友秘密不知道該不該分手...</span></a>\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(\"https://www.dcard.tw/f/relationship\")\n",
    "# html.parser 為解析 HTML 文件的模組\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "# 抓取按鈕群\n",
    "btn = soup.select('h2 > a')\n",
    "# 抓取上一頁按鈕的網址\n",
    "print(btn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "我男友跟我姐聊的很開心，還問她生日是什麼時候，直到剛剛我看到我姐的訊息（她都會讓我看），有很多則訊息 其中一則上面寫：其實是想見妳，我看到直接一直發抖 也哭不出來，他跟我姐聊的比跟我聊的還多"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('ml_torch': conda)",
   "language": "python",
   "name": "python38264bitmltorchconda00017c2c38fa48acbf54500a2f274542"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
